# NCCoE Practice Guide Vetting - Final Report
## Executive Summary
[comment]: # (We can copy and paste our original executive summary and update as needed.)
As rising security practitioners, we understand the need for quality guides explaining the how and why for best security practices. The NCCoE publishes the NIST SP 1800 series, which are a series of how-to guides that allow companies to implement various security policies in a step-by-step manner. These guides are designed for use by security professionals but may also be utilized by a general IT practitioner tasked with deploying the security implementation. Therefore, it is critical these guides are accurate and concise. For this project, the deliverable will be a grading scale that can be used to score the accuracy and conciseness of said documents. In particular, we’ll be implementing and focusing on DNS-Based Secure Email NIST SP 1800-6 Practice Guide. But, this grading scale can be used for any of the NIST SP 1800 series.

The ability to objectively compare readability, usability, consistency, and accuracy of the different NIST SP 1800 series documents is crucial, because user feedback can be difficult to quantify. Users often possess different technical backgrounds, expertise, and intelligence. This inevitably leads to different opinions on what constitutes a “good” guide. For example, these guides are used by both experienced security technicians and regular IT personnel. Obviously, a security technician would likely be more lenient on their usability scores, as they would likely have the background necessary to comprehend some of the technical jargon that is used in the guides.

Not only is the comparison of the documents important, but grading these documents with greater specificity will allow those tasked with improving the documents to focus their efforts on categories with lower scores. For example, if a certain NIST SP 1800 series document scores poorly in the category of readability, then it wouldn’t make much sense to have a highly technical person try and improve the document. Perhaps, efforts would be better served to have someone who specializes in technical communication work on the document so as not to waste resources.

Finally, poorly scored documents likely will not be used by companies wanting to implement that document’s specific security policy. So, efforts could be focused on improving that document in order to allow the implementation of that security policy to become more widespread. For instance, if the DNS-Based Secure Email NIST SP 1800-6 Practice Guide is scored poorly, then it would make sense to focus on the improvement of that document in order to allow companies to better implement the security policies proposed in the document.

## Project Goals
[comment]: # (We can copy and paste our original executive summary and update as needed.)
* Use the how-to guide to implement the security policy.
* Document any errors we find while using the how-to guide.
* Develop a grading scale for use on the entire NIST SP 1800 series based on our use of the DNS-Based Secure Email NIST SP 1800-6 Practice Guide.
* Explain and justify our grading scale criteria

## Project Methodology
### Readability Metric
[comment]: # (Methodology of applying readability metric)
### Usability Metric
[comment]: # (Methodology of applying usability metric)
### Consistency Metric
[comment]: # (Methodology of applying consistency metric)
### Replicating Practice Guide Example
[comment]: # (Methodology of replicating practice guide example)

## Results / Findings
[comment]: # (brief overview of outcomes - bulleted list of milestone 1/2/3 outcomes and any addl' outcomes)
* Obtained readable.io subscription
* Obtained readability score from readable.io
* Obtained Grammarly error report
* Developed questions for Practice Guide Usability Scale (PGUS) survey
* Developed "Familiarity With Technology" questions
  * Questions added to PGUS survey
* Created mock survey data for PGUS survey
* Configured reporting site for PGUS survey results
* Proposed methodology for disseminating PGUS survey to the public 
* Developed Consistency Rubric
* Completed set up of base lab environment
  * Obtained a domina name (capstone.uomaha.com)
  * Obtained public IP via cloud hosting
 * Replicated practice guide
  * Calculated accuracy score
  * Recorded areas of guide that felt vague and/or incomplete

## How to Use the Grading Scale
[comment]: # (This section replaces the "Install Instructions" section of the suggested format from Dr. Hale.)
In order to use the grading survey we’ve created, there are a number of steps that will need to be taken. The ten likert questions from the survey are homogenous across all practice guides. Familiarity with technology questions will need to be created specifically for the practice guide in question. The following steps need to be taken in order:

### Creating the PGUS
1. Follow the instructions to create technical questions found in the TechnicalSurvey folder. This is probably best done by those who have created the practice guide.
2. Prepend these questions to the PGUS questions.
3. Ensure the PGUS (with the technical questions) can be easily referenced alongside the practice guide.

### After Using the Guide
1. Have users answer the PGUS questions.
2. Calculate the PGUS results as specified in the UsabilityScoring folder.
3. Once enough people have taken the PGUS, there should be enough data to draw conclusions about how the practice guide should be changed.
4. Change the practice guide appropriately.
5. Have more users use and answer the PGUS questions.

### Process For Applying All Metrics

<img src="./misc/ApplyingMetrics.PNG" />
